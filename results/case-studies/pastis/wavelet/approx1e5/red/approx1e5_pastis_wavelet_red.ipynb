{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup, Loading Data and CDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_NAME = 'approx1e5-pastis-wavelet-blue' # Dataset Format: size-name-transform-channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m path_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel\u001b[39m\u001b[38;5;124m'\u001b[39m], DATA_NAME\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m      7\u001b[0m CWD \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(ROOT_DIR, os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresults\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcase-studies\u001b[39m\u001b[38;5;124m\"\u001b[39m, path_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m], path_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtransform\u001b[39m\u001b[38;5;124m'\u001b[39m], path_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msize\u001b[39m\u001b[38;5;124m'\u001b[39m], path_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchannel\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m CWD \u001b[38;5;241m==\u001b[39m os\u001b[38;5;241m.\u001b[39mgetcwd()\n\u001b[0;32m      9\u001b[0m Path(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(CWD, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCSVs\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m Path(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(CWD, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplots\u001b[39m\u001b[38;5;124m\"\u001b[39m))\u001b[38;5;241m.\u001b[39mmkdir(exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import git\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "ROOT_DIR = Path(git.Repo('.', search_parent_directories=True).working_tree_dir)\n",
    "path_dict = dict(zip(['size', 'name', 'transform', 'channel'], DATA_NAME.split(\"-\")))\n",
    "CWD = os.path.join(ROOT_DIR, os.path.join(\"results\", \"case-studies\", path_dict['name'], path_dict['transform'], path_dict['size'], path_dict['channel']))\n",
    "assert CWD == os.getcwd()\n",
    "Path(os.path.join(CWD, \"CSVs\")).mkdir(exist_ok=True)\n",
    "Path(os.path.join(CWD, \"plots\")).mkdir(exist_ok=True)\n",
    "Path(os.path.join(CWD, \"cache\")).mkdir(exist_ok=True)\n",
    "Path(os.path.join(CWD, \"groupCDFs\")).mkdir(exist_ok=True)\n",
    "\n",
    "GROUP = 'layer' if path_dict['transform'] == 'wavelet' else ('band' if path_dict['transform'] == 'fourier' else 'error')\n",
    "RERUN = False\n",
    "CWD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.join(ROOT_DIR, \"utilities\"))\n",
    "from testing import * # If MATLAB is not installed, open utilities and set to False\n",
    "from plotting import *\n",
    "os.chdir(CWD)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 3180, 3: 12720, 4: 50880, 5: 203520, 6: 814080, 7: 3256320, 8: 13025280}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_data_map = pd.read_pickle(os.path.join(ROOT_DIR, \"transformed-data\", f'{DATA_NAME}.pickle'))\n",
    "group_total_samples = pd.read_pickle(os.path.join(ROOT_DIR, \"transformed-data\", f'{DATA_NAME}-size.pickle'))\n",
    "\n",
    "if path_dict['transform'] == 'fourier':\n",
    "    GROUPS = np.arange(2, sorted(group_data_map)[-1] + 1)[::3]\n",
    "elif path_dict['transform'] == 'wavelet':\n",
    "    GROUPS = np.arange(2, sorted(group_data_map)[-1] + 1)\n",
    "\n",
    "cdfs_dir = os.path.join(ROOT_DIR, \"results\", \"CDFs\")\n",
    "cdfs_list = [os.path.join(cdfs_dir, i) for i in os.listdir(cdfs_dir)]\n",
    "all_cdfs = combine_pickles(cdfs_list[0])\n",
    "for cdf_dir in cdfs_list[1:]:\n",
    "    all_cdfs = all_cdfs | combine_pickles(cdf_dir)\n",
    "    \n",
    "group_data_map = {g : group_data_map[g][::1000] for g in GROUPS} # For quick testing purposes\n",
    "group_total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>(r,eta),cdf</th>\n",
       "      <th>r</th>\n",
       "      <th>eta</th>\n",
       "      <th>cdf</th>\n",
       "      <th>variance</th>\n",
       "      <th>kurtosis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>((0.02, -1.4), &lt;scipy.interpolate._fitpack2.In...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>&lt;scipy.interpolate._fitpack2.InterpolatedUniva...</td>\n",
       "      <td>9.618487e+69</td>\n",
       "      <td>1.391529e+25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>((0.02, -1.3), &lt;scipy.interpolate._fitpack2.In...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-1.3</td>\n",
       "      <td>&lt;scipy.interpolate._fitpack2.InterpolatedUniva...</td>\n",
       "      <td>3.821735e+74</td>\n",
       "      <td>8.172634e+21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>((0.02, -1.2), &lt;scipy.interpolate._fitpack2.In...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>&lt;scipy.interpolate._fitpack2.InterpolatedUniva...</td>\n",
       "      <td>1.455488e+78</td>\n",
       "      <td>4.131785e+19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>((0.02, -1.1), &lt;scipy.interpolate._fitpack2.In...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>&lt;scipy.interpolate._fitpack2.InterpolatedUniva...</td>\n",
       "      <td>1.406735e+81</td>\n",
       "      <td>6.947275e+17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>((0.02, -1.0), &lt;scipy.interpolate._fitpack2.In...</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>&lt;scipy.interpolate._fitpack2.InterpolatedUniva...</td>\n",
       "      <td>5.331443e+83</td>\n",
       "      <td>2.562075e+16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         (r,eta),cdf     r  eta  \\\n",
       "0  ((0.02, -1.4), <scipy.interpolate._fitpack2.In...  0.02 -1.4   \n",
       "1  ((0.02, -1.3), <scipy.interpolate._fitpack2.In...  0.02 -1.3   \n",
       "2  ((0.02, -1.2), <scipy.interpolate._fitpack2.In...  0.02 -1.2   \n",
       "3  ((0.02, -1.1), <scipy.interpolate._fitpack2.In...  0.02 -1.1   \n",
       "4  ((0.02, -1.0), <scipy.interpolate._fitpack2.In...  0.02 -1.0   \n",
       "\n",
       "                                                 cdf      variance  \\\n",
       "0  <scipy.interpolate._fitpack2.InterpolatedUniva...  9.618487e+69   \n",
       "1  <scipy.interpolate._fitpack2.InterpolatedUniva...  3.821735e+74   \n",
       "2  <scipy.interpolate._fitpack2.InterpolatedUniva...  1.455488e+78   \n",
       "3  <scipy.interpolate._fitpack2.InterpolatedUniva...  1.406735e+81   \n",
       "4  <scipy.interpolate._fitpack2.InterpolatedUniva...  5.331443e+83   \n",
       "\n",
       "       kurtosis  \n",
       "0  1.391529e+25  \n",
       "1  8.172634e+21  \n",
       "2  4.131785e+19  \n",
       "3  6.947275e+17  \n",
       "4  2.562075e+16  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "\n",
    "all_cdfs_df = create_kurt_var_ksstat_df(all_cdfs)\n",
    "coarse_cdf_df = all_cdfs_df[(np.round(all_cdfs_df['r'], 0) == all_cdfs_df['r']) & (np.round(all_cdfs_df['eta'], 0) == all_cdfs_df['eta'])]\n",
    "var_values_dict = dict()\n",
    "kurt_values_dict = dict()\n",
    "master_df = pd.DataFrame(columns=[GROUP]).set_index(GROUP)\n",
    "temp_cdf = all_cdfs_df\n",
    "all_cdfs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping the Variance and Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bootstrap = int(1e2)\n",
    "bootstrap_size = int(1e2)\n",
    "ci = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:15<00:00, 6512.85it/s]\n",
      "100%|██████████| 100000/100000 [01:02<00:00, 1590.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:45<00:00, 2203.24it/s]\n",
      "100%|██████████| 100000/100000 [04:23<00:00, 379.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:32<00:00, 3068.78it/s]\n",
      "100%|██████████| 100000/100000 [01:43<00:00, 961.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:30<00:00, 3254.19it/s]\n",
      "100%|██████████| 100000/100000 [01:34<00:00, 1060.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:31<00:00, 3209.41it/s]\n",
      "100%|██████████| 100000/100000 [01:38<00:00, 1018.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:30<00:00, 3298.74it/s]\n",
      "100%|██████████| 100000/100000 [01:34<00:00, 1054.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:30<00:00, 3238.47it/s]\n",
      "100%|██████████| 100000/100000 [01:34<00:00, 1054.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obs_var</th>\n",
       "      <th>var_lower</th>\n",
       "      <th>var_upper</th>\n",
       "      <th>obs_kurt</th>\n",
       "      <th>kurt_lower</th>\n",
       "      <th>kurt_upper</th>\n",
       "      <th>total_samples</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2543.706000</td>\n",
       "      <td>2322.266800</td>\n",
       "      <td>2776.369900</td>\n",
       "      <td>1.855184</td>\n",
       "      <td>1.231146</td>\n",
       "      <td>2.552742</td>\n",
       "      <td>3180.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>880.942400</td>\n",
       "      <td>837.707600</td>\n",
       "      <td>925.666000</td>\n",
       "      <td>1.832932</td>\n",
       "      <td>1.492253</td>\n",
       "      <td>2.208135</td>\n",
       "      <td>12720.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>249.724290</td>\n",
       "      <td>236.018890</td>\n",
       "      <td>263.695700</td>\n",
       "      <td>2.604847</td>\n",
       "      <td>2.158933</td>\n",
       "      <td>3.140816</td>\n",
       "      <td>50880.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56.870796</td>\n",
       "      <td>53.349483</td>\n",
       "      <td>60.484978</td>\n",
       "      <td>4.033042</td>\n",
       "      <td>3.357861</td>\n",
       "      <td>4.833753</td>\n",
       "      <td>203520.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.734995</td>\n",
       "      <td>9.960573</td>\n",
       "      <td>11.537709</td>\n",
       "      <td>5.936091</td>\n",
       "      <td>4.932771</td>\n",
       "      <td>7.195107</td>\n",
       "      <td>814080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.834359</td>\n",
       "      <td>1.688523</td>\n",
       "      <td>1.988222</td>\n",
       "      <td>8.154408</td>\n",
       "      <td>6.703937</td>\n",
       "      <td>10.309594</td>\n",
       "      <td>3256320.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.259103</td>\n",
       "      <td>0.236675</td>\n",
       "      <td>0.283873</td>\n",
       "      <td>10.494855</td>\n",
       "      <td>7.995214</td>\n",
       "      <td>17.801400</td>\n",
       "      <td>13025280.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           obs_var    var_lower    var_upper   obs_kurt  kurt_lower  \\\n",
       "layer                                                                 \n",
       "2      2543.706000  2322.266800  2776.369900   1.855184    1.231146   \n",
       "3       880.942400   837.707600   925.666000   1.832932    1.492253   \n",
       "4       249.724290   236.018890   263.695700   2.604847    2.158933   \n",
       "5        56.870796    53.349483    60.484978   4.033042    3.357861   \n",
       "6        10.734995     9.960573    11.537709   5.936091    4.932771   \n",
       "7         1.834359     1.688523     1.988222   8.154408    6.703937   \n",
       "8         0.259103     0.236675     0.283873  10.494855    7.995214   \n",
       "\n",
       "       kurt_upper  total_samples  \n",
       "layer                             \n",
       "2        2.552742         3180.0  \n",
       "3        2.208135        12720.0  \n",
       "4        3.140816        50880.0  \n",
       "5        4.833753       203520.0  \n",
       "6        7.195107       814080.0  \n",
       "7       10.309594      3256320.0  \n",
       "8       17.801400     13025280.0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bootstrap_path = Path(os.path.join(CWD, \"CSVs\", f'initial_grid_bootstrap{n_bootstrap}_{bootstrap_size}_ci{ci}.csv'))\n",
    "master_df_var_kurt_path = Path(os.path.join(CWD, \"CSVs\", f'initial_grid_bootstrap{n_bootstrap}_{bootstrap_size}_ci{ci}.csv'))\n",
    "\n",
    "if RERUN or not master_df_var_kurt_path.exists():\n",
    "    for group in GROUPS:\n",
    "        print(f'{GROUP.capitalize()} {group}')\n",
    "        obs_var, var_lower, var_upper, var_values_dict[group] = bootstrap_metric(group_data_map[group], \n",
    "                                                                                n_bootstrap=n_bootstrap, \n",
    "                                                                                bootstrap_size=min(group_data_map[group].size, bootstrap_size), \n",
    "                                                                                metric= np.var, \n",
    "                                                                                ci=ci)\n",
    "        obs_kurt, kurt_lower, kurt_upper, kurt_values_dict[group] = bootstrap_metric(group_data_map[group], \n",
    "                                                                                    n_bootstrap=n_bootstrap, \n",
    "                                                                                    bootstrap_size=min(group_data_map[group].size, bootstrap_size), \n",
    "                                                                                    metric= stats.kurtosis, ci=ci)  \n",
    "        master_df.loc[group, 'obs_var'], master_df.loc[group, 'var_lower'], master_df.loc[group, 'var_upper'] = obs_var, var_lower, var_upper\n",
    "        master_df.loc[group, 'obs_kurt'], master_df.loc[group, 'kurt_lower'], master_df.loc[group, 'kurt_upper'] = obs_kurt, kurt_lower, kurt_upper\n",
    "        master_df.loc[group, 'total_samples'] = group_total_samples[group]\n",
    "\n",
    "    master_df.to_csv(os.path.join(CWD, \"CSVs\", f'initial_grid_bootstrap{n_bootstrap}_{bootstrap_size}_ci{ci}.csv'))\n",
    "\n",
    "master_df = pd.read_csv(master_df_var_kurt_path, index_col=GROUP)\n",
    "var_kurt_df = pd.read_csv(bootstrap_path, index_col=GROUP)\n",
    "master_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Grid Search and Hypothesis Test Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obs_var</th>\n",
       "      <th>var_lower</th>\n",
       "      <th>var_upper</th>\n",
       "      <th>obs_kurt</th>\n",
       "      <th>kurt_lower</th>\n",
       "      <th>kurt_upper</th>\n",
       "      <th>total_samples</th>\n",
       "      <th>initial_r</th>\n",
       "      <th>initial_eta</th>\n",
       "      <th>initial_scale</th>\n",
       "      <th>kstest_stat_initial</th>\n",
       "      <th>kstest_stat_cutoff_0.05</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2543.706000</td>\n",
       "      <td>2322.266800</td>\n",
       "      <td>2776.369900</td>\n",
       "      <td>1.855184</td>\n",
       "      <td>1.231146</td>\n",
       "      <td>2.552742</td>\n",
       "      <td>3180.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>2.90</td>\n",
       "      <td>5.617178e-01</td>\n",
       "      <td>0.015555</td>\n",
       "      <td>0.024030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>880.942400</td>\n",
       "      <td>837.707600</td>\n",
       "      <td>925.666000</td>\n",
       "      <td>1.832932</td>\n",
       "      <td>1.492253</td>\n",
       "      <td>2.208135</td>\n",
       "      <td>12720.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>4.867665e+02</td>\n",
       "      <td>0.004411</td>\n",
       "      <td>0.012029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>249.724290</td>\n",
       "      <td>236.018890</td>\n",
       "      <td>263.695700</td>\n",
       "      <td>2.604847</td>\n",
       "      <td>2.158933</td>\n",
       "      <td>3.140816</td>\n",
       "      <td>50880.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>9.324044e+01</td>\n",
       "      <td>0.004853</td>\n",
       "      <td>0.006018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56.870796</td>\n",
       "      <td>53.349483</td>\n",
       "      <td>60.484978</td>\n",
       "      <td>4.033042</td>\n",
       "      <td>3.357861</td>\n",
       "      <td>4.833753</td>\n",
       "      <td>203520.0</td>\n",
       "      <td>0.61</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>1.946623e+01</td>\n",
       "      <td>0.002439</td>\n",
       "      <td>0.003010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.734995</td>\n",
       "      <td>9.960573</td>\n",
       "      <td>11.537709</td>\n",
       "      <td>5.936091</td>\n",
       "      <td>4.932771</td>\n",
       "      <td>7.195107</td>\n",
       "      <td>814080.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>5.713932e-01</td>\n",
       "      <td>0.003293</td>\n",
       "      <td>0.001505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.834359</td>\n",
       "      <td>1.688523</td>\n",
       "      <td>1.988222</td>\n",
       "      <td>8.154408</td>\n",
       "      <td>6.703937</td>\n",
       "      <td>10.309594</td>\n",
       "      <td>3256320.0</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>2.619346e-04</td>\n",
       "      <td>0.006394</td>\n",
       "      <td>0.000753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.259103</td>\n",
       "      <td>0.236675</td>\n",
       "      <td>0.283873</td>\n",
       "      <td>10.494855</td>\n",
       "      <td>7.995214</td>\n",
       "      <td>17.801400</td>\n",
       "      <td>13025280.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.20</td>\n",
       "      <td>4.112338e-13</td>\n",
       "      <td>0.024760</td>\n",
       "      <td>0.000376</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           obs_var    var_lower    var_upper   obs_kurt  kurt_lower  \\\n",
       "layer                                                                 \n",
       "2      2543.706000  2322.266800  2776.369900   1.855184    1.231146   \n",
       "3       880.942400   837.707600   925.666000   1.832932    1.492253   \n",
       "4       249.724290   236.018890   263.695700   2.604847    2.158933   \n",
       "5        56.870796    53.349483    60.484978   4.033042    3.357861   \n",
       "6        10.734995     9.960573    11.537709   5.936091    4.932771   \n",
       "7         1.834359     1.688523     1.988222   8.154408    6.703937   \n",
       "8         0.259103     0.236675     0.283873  10.494855    7.995214   \n",
       "\n",
       "       kurt_upper  total_samples  initial_r  initial_eta  initial_scale  \\\n",
       "layer                                                                     \n",
       "2        2.552742         3180.0       0.32         2.90   5.617178e-01   \n",
       "3        2.208135        12720.0       0.90        -0.01   4.867665e+02   \n",
       "4        3.140816        50880.0       0.70        -0.24   9.324044e+01   \n",
       "5        4.833753       203520.0       0.61        -0.50   1.946623e+01   \n",
       "6        7.195107       814080.0       0.40        -0.48   5.713932e-01   \n",
       "7       10.309594      3256320.0       0.23        -0.10   2.619346e-04   \n",
       "8       17.801400     13025280.0       0.12         1.20   4.112338e-13   \n",
       "\n",
       "       kstest_stat_initial  kstest_stat_cutoff_0.05  \n",
       "layer                                                \n",
       "2                 0.015555                 0.024030  \n",
       "3                 0.004411                 0.012029  \n",
       "4                 0.004853                 0.006018  \n",
       "5                 0.002439                 0.003010  \n",
       "6                 0.003293                 0.001505  \n",
       "7                 0.006394                 0.000753  \n",
       "8                 0.024760                 0.000376  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df_combo_path = Path(os.path.join(CWD, \"CSVs\", \"master_df_combo.csv\"))\n",
    "\n",
    "if RERUN or not master_df_combo_path.exists():\n",
    "    sorted_params = sorted(all_cdfs)\n",
    "    group_cdf_df_dict = dict()\n",
    "    ksstats_dict = dict()\n",
    "\n",
    "    for i, group in enumerate(GROUPS):\n",
    "        print(f\"####\\n{GROUP.capitalize()} \", group)\n",
    "        sample = group_data_map[group]\n",
    "        group_cdf_df = all_cdfs_df.copy()\n",
    "        \n",
    "        group_cdf_df['scale'] = np.clip(master_df.loc[group,'obs_var'] / group_cdf_df['variance'], 0, 1e5)\n",
    "        ksstats, initial_param, min_stat = gridsearch(sample, all_cdfs, debug=True, scales=group_cdf_df['scale'])\n",
    "        initial_scale = group_cdf_df.loc[(group_cdf_df[\"r\"] ==  initial_param[0]) & (group_cdf_df[\"eta\"] ==  initial_param[1])][\"scale\"].iloc[0]\n",
    "        master_df.loc[group, 'initial_r'], master_df.loc[group, 'initial_eta'] = initial_param\n",
    "        master_df.loc[group, 'initial_scale'] = initial_scale\n",
    "        print(f\"Number of samples: {sample.size}, Without approximation : {master_df.loc[group, 'total_samples']}\")\n",
    "        master_df.loc[group, 'kstest_stat_initial'] = min_stat\n",
    "        cutoff = stats.kstwo(n=master_df.loc[group, 'total_samples']).isf(0.05)\n",
    "        master_df.loc[group, 'kstest_stat_cutoff_0.05'] = cutoff\n",
    "\n",
    "        group_cdf_df['variance'] = group_cdf_df['variance'] * group_cdf_df['scale']\n",
    "        group_cdf_df['kurtosis'] = group_cdf_df['kurtosis'] * group_cdf_df['scale']\n",
    "\n",
    "        group_cdf_df = add_tests_to_df(cdfs_df = group_cdf_df, group = group, var_kurt_df = master_df, ksstats = ksstats)\n",
    "        group_cdf_df_dict[group] = group_cdf_df\n",
    "\n",
    "    master_df.to_csv(os.path.join(CWD, \"CSVs\", \"master_df_combo.csv\"))\n",
    "\n",
    "master_df = pd.read_csv(master_df_combo_path, index_col=GROUP)\n",
    "master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(GROUPS):\n\u001b[0;32m      3\u001b[0m         cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpass_var\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpass_kstest\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpass_kurt\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 5\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mgroup_cdf_df_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpass_kstest\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m&\u001b[39m group_cdf_df_dict[group][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpass_kurt\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      6\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28msum\u001b[39m(group_cdf_df_dict[group][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpass_kstest\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[0;32m      7\u001b[0m                 cutoff \u001b[38;5;241m=\u001b[39m stats\u001b[38;5;241m.\u001b[39mkstwo(n\u001b[38;5;241m=\u001b[39mvar_kurt_df\u001b[38;5;241m.\u001b[39mloc[group, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_samples\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39misf(\u001b[38;5;241m0.05\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: 2"
     ]
    }
   ],
   "source": [
    "for i, group in enumerate(GROUPS):\n",
    "\n",
    "        cols = ['pass_var', 'pass_kstest', 'pass_kurt']\n",
    "        \n",
    "        if sum(group_cdf_df_dict[group]['pass_kstest'] & group_cdf_df_dict[group]['pass_kurt']) == 0:\n",
    "                print(sum(group_cdf_df_dict[group]['pass_kstest']))\n",
    "                cutoff = stats.kstwo(n=var_kurt_df.loc[group, 'total_samples']).isf(0.10)\n",
    "                group_cdf_df_dict[group]['pass_kstest'] = group_cdf_df_dict[group]['ksstat'].apply(lambda x: True if x < cutoff else False)\n",
    "                print(group, \"after\", sum(group_cdf_df_dict[group]['pass_kstest']))\n",
    "\n",
    "                fig = combo_test_plot(group_cdf_df_dict[group], cols, \n",
    "                                plot_name=f\"{GROUP.capitalize()} {group}: {', '.join([col[5:].capitalize() for col in cols])} (alpha=0.10)\", \n",
    "                                target_var = None,\n",
    "                                best_param = (master_df.loc[group, 'initial_r'], master_df.loc[group, 'initial_eta']),\n",
    "                                best_ksstat= master_df.loc[group, 'kstest_stat_initial']\n",
    "                                )\n",
    "        else: \n",
    "                fig = combo_test_plot(group_cdf_df_dict[group], cols, \n",
    "                                plot_name=f\"{GROUP.capitalize()} {group}: {', '.join([col[5:].capitalize() for col in cols])} (alpha=0.05)\", \n",
    "                                target_var = None,\n",
    "                                best_param = (master_df.loc[group, 'initial_r'], master_df.loc[group, 'initial_eta']),\n",
    "                                best_ksstat= master_df.loc[group, 'kstest_stat_initial']\n",
    "                                )\n",
    "        \n",
    "\n",
    "        fig.figure.savefig(os.path.join(CWD, \"plots\", f\"full_grid_search_combo_plot_layer{group}.jpg\"), bbox_inches = 'tight', dpi=600)\n",
    "        \n",
    "        \n",
    "        # Optional: \n",
    "        # Create plots of bootstrapped variance and kurtosis for varying confidence intervals\n",
    "        # fig_var = create_ci_scatter_plot(group_cdf_df_dict[group], var_values_dict, metric='variance', group=group)\n",
    "        # fig_kurt = create_ci_scatter_plot(group_cdf_df_dict[group], kurt_values_dict, metric='kurtosis', group=group)\n",
    "\n",
    "        # fig_var.savefig(os.path.join(CWD, \"plots\", f\"ci_scatter_variance_{GROUP}_{group}_bootstrap{n_bootstrap}.jpg\"), bbox_inches='tight')\n",
    "        # plt.close(fig_var)\n",
    "        # fig_kurt.savefig(os.path.join(CWD, \"plots\", f\"ci_scatter_variance_{GROUP}_{group}_bootstrap{n_bootstrap}.jpg\"), bbox_inches='tight')\n",
    "        # plt.close(fig_kurt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search over $\\eta=0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 369/369 [00:00<00:00, 2571.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding Minimum after computing 369 CDFs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 369/369 [00:00<00:00, 567.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding Minimum after computing 369 CDFs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 369/369 [00:03<00:00, 108.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding Minimum after computing 369 CDFs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 369/369 [00:06<00:00, 60.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding Minimum after computing 369 CDFs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 369/369 [00:05<00:00, 62.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding Minimum after computing 369 CDFs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 369/369 [00:04<00:00, 80.07it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding Minimum after computing 369 CDFs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 369/369 [00:05<00:00, 72.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding Minimum after computing 369 CDFs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>obs_var</th>\n",
       "      <th>var_lower</th>\n",
       "      <th>var_upper</th>\n",
       "      <th>obs_kurt</th>\n",
       "      <th>kurt_lower</th>\n",
       "      <th>kurt_upper</th>\n",
       "      <th>total_samples</th>\n",
       "      <th>initial_r</th>\n",
       "      <th>initial_eta</th>\n",
       "      <th>initial_scale</th>\n",
       "      <th>kstest_stat_initial</th>\n",
       "      <th>kstest_stat_cutoff_0.05</th>\n",
       "      <th>best_r_eta0</th>\n",
       "      <th>best_scale_eta0</th>\n",
       "      <th>kstest_stat_eta0</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>layer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2543.706000</td>\n",
       "      <td>2322.266800</td>\n",
       "      <td>2776.369900</td>\n",
       "      <td>1.855184</td>\n",
       "      <td>1.231146</td>\n",
       "      <td>2.552742</td>\n",
       "      <td>3180.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>2.90</td>\n",
       "      <td>5.617178e-01</td>\n",
       "      <td>0.015555</td>\n",
       "      <td>0.024030</td>\n",
       "      <td>6.00</td>\n",
       "      <td>4.415424e-01</td>\n",
       "      <td>0.304830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>880.942400</td>\n",
       "      <td>837.707600</td>\n",
       "      <td>925.666000</td>\n",
       "      <td>1.832932</td>\n",
       "      <td>1.492253</td>\n",
       "      <td>2.208135</td>\n",
       "      <td>12720.0</td>\n",
       "      <td>0.90</td>\n",
       "      <td>-0.01</td>\n",
       "      <td>4.867665e+02</td>\n",
       "      <td>0.004411</td>\n",
       "      <td>0.012029</td>\n",
       "      <td>5.90</td>\n",
       "      <td>4.412491e-01</td>\n",
       "      <td>0.361339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>249.724290</td>\n",
       "      <td>236.018890</td>\n",
       "      <td>263.695700</td>\n",
       "      <td>2.604847</td>\n",
       "      <td>2.158933</td>\n",
       "      <td>3.140816</td>\n",
       "      <td>50880.0</td>\n",
       "      <td>0.70</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>9.324044e+01</td>\n",
       "      <td>0.004853</td>\n",
       "      <td>0.006018</td>\n",
       "      <td>5.80</td>\n",
       "      <td>4.409327e-01</td>\n",
       "      <td>0.411001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>56.870796</td>\n",
       "      <td>53.349483</td>\n",
       "      <td>60.484978</td>\n",
       "      <td>4.033042</td>\n",
       "      <td>3.357861</td>\n",
       "      <td>4.833753</td>\n",
       "      <td>203520.0</td>\n",
       "      <td>0.61</td>\n",
       "      <td>-0.50</td>\n",
       "      <td>1.946623e+01</td>\n",
       "      <td>0.002439</td>\n",
       "      <td>0.003010</td>\n",
       "      <td>5.80</td>\n",
       "      <td>4.409327e-01</td>\n",
       "      <td>0.447530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.734995</td>\n",
       "      <td>9.960573</td>\n",
       "      <td>11.537709</td>\n",
       "      <td>5.936091</td>\n",
       "      <td>4.932771</td>\n",
       "      <td>7.195107</td>\n",
       "      <td>814080.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>5.713932e-01</td>\n",
       "      <td>0.003293</td>\n",
       "      <td>0.001505</td>\n",
       "      <td>5.80</td>\n",
       "      <td>4.409327e-01</td>\n",
       "      <td>0.471578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.834359</td>\n",
       "      <td>1.688523</td>\n",
       "      <td>1.988222</td>\n",
       "      <td>8.154408</td>\n",
       "      <td>6.703937</td>\n",
       "      <td>10.309594</td>\n",
       "      <td>3256320.0</td>\n",
       "      <td>0.23</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>2.619346e-04</td>\n",
       "      <td>0.006394</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>5.80</td>\n",
       "      <td>4.409327e-01</td>\n",
       "      <td>0.485369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.259103</td>\n",
       "      <td>0.236675</td>\n",
       "      <td>0.283873</td>\n",
       "      <td>10.494855</td>\n",
       "      <td>7.995214</td>\n",
       "      <td>17.801400</td>\n",
       "      <td>13025280.0</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.20</td>\n",
       "      <td>4.112338e-13</td>\n",
       "      <td>0.024760</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.03</td>\n",
       "      <td>7.610968e-62</td>\n",
       "      <td>0.477730</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           obs_var    var_lower    var_upper   obs_kurt  kurt_lower  \\\n",
       "layer                                                                 \n",
       "2      2543.706000  2322.266800  2776.369900   1.855184    1.231146   \n",
       "3       880.942400   837.707600   925.666000   1.832932    1.492253   \n",
       "4       249.724290   236.018890   263.695700   2.604847    2.158933   \n",
       "5        56.870796    53.349483    60.484978   4.033042    3.357861   \n",
       "6        10.734995     9.960573    11.537709   5.936091    4.932771   \n",
       "7         1.834359     1.688523     1.988222   8.154408    6.703937   \n",
       "8         0.259103     0.236675     0.283873  10.494855    7.995214   \n",
       "\n",
       "       kurt_upper  total_samples  initial_r  initial_eta  initial_scale  \\\n",
       "layer                                                                     \n",
       "2        2.552742         3180.0       0.32         2.90   5.617178e-01   \n",
       "3        2.208135        12720.0       0.90        -0.01   4.867665e+02   \n",
       "4        3.140816        50880.0       0.70        -0.24   9.324044e+01   \n",
       "5        4.833753       203520.0       0.61        -0.50   1.946623e+01   \n",
       "6        7.195107       814080.0       0.40        -0.48   5.713932e-01   \n",
       "7       10.309594      3256320.0       0.23        -0.10   2.619346e-04   \n",
       "8       17.801400     13025280.0       0.12         1.20   4.112338e-13   \n",
       "\n",
       "       kstest_stat_initial  kstest_stat_cutoff_0.05  best_r_eta0  \\\n",
       "layer                                                              \n",
       "2                 0.015555                 0.024030         6.00   \n",
       "3                 0.004411                 0.012029         5.90   \n",
       "4                 0.004853                 0.006018         5.80   \n",
       "5                 0.002439                 0.003010         5.80   \n",
       "6                 0.003293                 0.001505         5.80   \n",
       "7                 0.006394                 0.000753         5.80   \n",
       "8                 0.024760                 0.000376         0.03   \n",
       "\n",
       "       best_scale_eta0  kstest_stat_eta0  \n",
       "layer                                     \n",
       "2         4.415424e-01          0.304830  \n",
       "3         4.412491e-01          0.361339  \n",
       "4         4.409327e-01          0.411001  \n",
       "5         4.409327e-01          0.447530  \n",
       "6         4.409327e-01          0.471578  \n",
       "7         4.409327e-01          0.485369  \n",
       "8         7.610968e-62          0.477730  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df_eta0_path = Path(os.path.join(CWD, \"CSVs\", \"master_df_eta0.csv\"))\n",
    "\n",
    "if RERUN or not master_df_eta0_path.exists():\n",
    "    best_params_eta0 = []\n",
    "    for i, group in enumerate(GROUPS):\n",
    "        if master_df.loc[group, 'initial_eta'] != 0:\n",
    "            sample = group_data_map[group]\n",
    "            group_cdf = all_cdfs_df[all_cdfs_df['eta'] == 0]\n",
    "            cdfs_dict = {i[0]:i[1] for i in group_cdf['(r,eta),cdf']}\n",
    "            ksstats, best_param_eta0, kstest_stat_eta0 = gridsearch(sample, cdfs_dict, debug=True, scales=group_cdf_df['scale'])\n",
    "            best_scale_eta0 = group_cdf_df.loc[(group_cdf_df[\"r\"] ==  best_param_eta0[0]) & (group_cdf_df[\"eta\"] ==  best_param_eta0[1])][\"scale\"].iloc[0]\n",
    "            master_df.loc[group, 'best_r_eta0'] = best_param_eta0[0]\n",
    "            master_df.loc[group, 'best_scale_eta0'] = best_scale_eta0\n",
    "            master_df.loc[group, 'kstest_stat_eta0'] = kstest_stat_eta0\n",
    "        else:\n",
    "            master_df.loc[group, 'kstest_stat_eta0'] = master_df.loc[group, 'kstest_stat_initial']\n",
    "            master_df.loc[group, 'best_r_eta0'] = master_df.loc[group, 'initial_r']\n",
    "            master_df.loc[group, 'best_scale_eta0'] = master_df.loc[group, 'initial_scale']\n",
    "    master_df.to_csv(os.path.join(CWD, \"CSVs\", \"master_df_eta0.csv\"))\n",
    "\n",
    "master_df = pd.read_csv(master_df_eta0_path, index_col=GROUP)\n",
    "master_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_add_cdfs(r_range, eta_range, dir, folder_name = '', n_samples = 500, tail_bound = 0.01, tail_percent = 0.1, enforce_assert=True, return_assert = False, debug=False):\n",
    "\n",
    "    if folder_name == '':\n",
    "        folder_name = f'r{round_to_sigfigs(min(r_range))}to{round_to_sigfigs(max(r_range))}_eta{round_to_sigfigs(min(eta_range))}to{round_to_sigfigs(max(eta_range))}'\n",
    "\n",
    "    FOLDER_PATH = os.path.join(dir, folder_name)\n",
    "\n",
    "    if os.path.isdir(FOLDER_PATH):\n",
    "        cdfs_completed = combine_pickles(FOLDER_PATH)\n",
    "        if debug:\n",
    "            print(\"CDFs completed:\", len(cdfs_completed))\n",
    "    else:\n",
    "        Path(os.path.join(os.getcwd(), FOLDER_PATH)).mkdir()\n",
    "        cdfs_completed = dict()\n",
    "\n",
    "    n = len(r_range)*len(eta_range)\n",
    "    finished = len(cdfs_completed)\n",
    "    cnt = len(cdfs_completed)\n",
    "    for r in r_range:\n",
    "        r_cdf = dict()\n",
    "        r = round_to_sigfigs(r)\n",
    "        for eta in eta_range:\n",
    "            eta = round_to_sigfigs(eta)\n",
    "            if ((r, eta) in cdfs_completed) and cdfs_completed[(r, eta)]:\n",
    "                continue\n",
    "            cnt += 1\n",
    "            if debug:\n",
    "                print(f'{(r, eta)}, {cnt} of {n + finished}')\n",
    "            if cnt % 50 == 0:\n",
    "                print(f'{(r, eta)}, {cnt} of {n + finished}')\n",
    "\n",
    "            computed_cdf = compute_prior_cdf(r = r, eta = eta, method = 'gamma_cdf', n_samples = n_samples, tail_percent = tail_percent, tail_bound = tail_bound, \n",
    "                                             enforce_assert=enforce_assert, return_assert=return_assert, debug=debug)\n",
    "            r_cdf[(r, eta)] = computed_cdf\n",
    "        if r_cdf:\n",
    "            sorted_r_cdf = [i[1] for i in sorted(r_cdf)]\n",
    "            min_eta, max_eta = round_to_sigfigs(min(sorted_r_cdf), 6), round_to_sigfigs(max(sorted_r_cdf), 6)\n",
    "            pkl_path = os.path.join(FOLDER_PATH, f'r{r}_eta{min_eta}to{max_eta}.pickle')\n",
    "            pd.to_pickle(r_cdf, pkl_path)\n",
    "        else:\n",
    "            if debug:\n",
    "                print(f\"Skipped {r} entirely\")\n",
    "\n",
    "    if debug:\n",
    "        print(f'You can find the CDFs here: {os.path.join(os.getcwd(), FOLDER_PATH)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 2\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "KS test failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 40\u001b[0m\n\u001b[0;32m     38\u001b[0m r_range \u001b[38;5;241m=\u001b[39m [round_to_sigfigs(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39marange(initial_r \u001b[38;5;241m-\u001b[39m r_granularity, initial_r \u001b[38;5;241m+\u001b[39m r_granularity, r_granularity\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10\u001b[39m)]\n\u001b[0;32m     39\u001b[0m eta_range \u001b[38;5;241m=\u001b[39m [round_to_sigfigs(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39marange(initial_eta \u001b[38;5;241m-\u001b[39m eta_granularity, initial_eta \u001b[38;5;241m+\u001b[39m eta_granularity, eta_granularity\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m10\u001b[39m)]\n\u001b[1;32m---> 40\u001b[0m \u001b[43msimple_add_cdfs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mdir\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCWD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgroupCDFs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfolder_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mGROUP\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mgroup\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mNUM_SAMPLES_OPTIMIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtail_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m cdfs_dict \u001b[38;5;241m=\u001b[39m combine_pickles(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(CWD, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupCDFs\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mGROUP\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mgroup\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     44\u001b[0m temp_df \u001b[38;5;241m=\u001b[39m create_kurt_var_ksstat_df(cdfs_dict)\n",
      "Cell \u001b[1;32mIn[14], line 32\u001b[0m, in \u001b[0;36msimple_add_cdfs\u001b[1;34m(r_range, eta_range, dir, folder_name, n_samples, tail_bound, tail_percent, enforce_assert, return_assert, debug)\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cnt \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m50\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(r,\u001b[38;5;250m \u001b[39meta)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcnt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39mfinished\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m     computed_cdf \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_prior_cdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgamma_cdf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtail_percent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtail_percent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtail_bound\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtail_bound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m                                     \u001b[49m\u001b[43menforce_assert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_assert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_assert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_assert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m     r_cdf[(r, eta)] \u001b[38;5;241m=\u001b[39m computed_cdf\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m r_cdf:\n",
      "File \u001b[1;32mc:\\Users\\yashd\\Desktop\\hierarchical-bayesian-model-validation\\utilities\\testing.py:68\u001b[0m, in \u001b[0;36mcompute_prior_cdf\u001b[1;34m(r, eta, method, n_samples, tail_bound, tail_percent, scale, use_matlab, eng, debug, enforce_assert, return_assert, return_xs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_prior_cdf\u001b[39m(r, eta, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma_cdf\u001b[39m\u001b[38;5;124m'\u001b[39m, n_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m, tail_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m, tail_percent \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m, scale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, use_matlab\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, eng\u001b[38;5;241m=\u001b[39meng, debug\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, enforce_assert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_assert\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, return_xs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma_cdf\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 68\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompute_prior_cdf_using_gamma_cdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtail_bound\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtail_bound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtail_percent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtail_percent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_matlab\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_matlab\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menforce_assert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_assert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_assert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_assert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_xs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_xs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnormal_cdf\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     70\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m compute_prior_cdf_using_normal_cdf(r\u001b[38;5;241m=\u001b[39mr, eta\u001b[38;5;241m=\u001b[39meta, n_samples\u001b[38;5;241m=\u001b[39mn_samples, tail_bound\u001b[38;5;241m=\u001b[39mtail_bound, tail_percent\u001b[38;5;241m=\u001b[39mtail_percent, scale\u001b[38;5;241m=\u001b[39mscale, use_matlab\u001b[38;5;241m=\u001b[39muse_matlab, eng\u001b[38;5;241m=\u001b[39meng, enforce_assert\u001b[38;5;241m=\u001b[39menforce_assert, return_assert\u001b[38;5;241m=\u001b[39mreturn_assert, return_xs\u001b[38;5;241m=\u001b[39mreturn_xs, debug\u001b[38;5;241m=\u001b[39mdebug)\n",
      "File \u001b[1;32mc:\\Users\\yashd\\Desktop\\hierarchical-bayesian-model-validation\\utilities\\testing.py:148\u001b[0m, in \u001b[0;36mcompute_prior_cdf_using_gamma_cdf\u001b[1;34m(r, eta, n_samples, tail_bound, tail_percent, scale, use_matlab, eng, enforce_assert, return_assert, return_xs, debug)\u001b[0m\n\u001b[0;32m    146\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m (xs, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mif\u001b[39;00m return_xs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    147\u001b[0m         \u001b[38;5;28;01melif\u001b[39;00m enforce_assert:\n\u001b[1;32m--> 148\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKS test failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (xs, cdf_spline) \u001b[38;5;28;01mif\u001b[39;00m return_xs \u001b[38;5;28;01melse\u001b[39;00m cdf_spline\n",
      "\u001b[1;31mAssertionError\u001b[0m: KS test failed"
     ]
    }
   ],
   "source": [
    "master_df_optimized_path = Path(os.path.join(CWD, \"CSVs\", 'master_df_optimized.csv'))\n",
    "rEtaKsstats_dict_path = Path(os.path.join(CWD, \"cache\", \"rEtaKsstats_dict.pickle\"))\n",
    "\n",
    "\n",
    "SKIP_OPTIMIZE_STEP = False\n",
    "NUM_ITERS = dict(zip(GROUPS, [1]*len(GROUPS)))\n",
    "NUM_SAMPLES_OPTIMIZE = 2000\n",
    "\n",
    "if RERUN or not master_df_optimized_path.exists():\n",
    "    rEtaKsstats_dict = dict()\n",
    "\n",
    "    if SKIP_OPTIMIZE_STEP:\n",
    "        master_df['kstest_stat_best'] = master_df['kstest_stat_initial']\n",
    "        master_df[f'best_r'] = master_df['initial_r']\n",
    "        master_df[f'best_eta'] = master_df['initial_eta']\n",
    "    else:\n",
    "        for group in GROUPS: \n",
    "            print(f\"{GROUP.capitalize()} {group}\")\n",
    "            sample = group_data_map[group]\n",
    "            initial_r, initial_eta = master_df.loc[group, 'initial_r'], master_df.loc[group, f'initial_eta']\n",
    "            r_granularity = 10\n",
    "            eta_granularity = 10\n",
    "            for i in range(0, -5, -1):\n",
    "                if tuple([round_to_sigfigs(initial_r + 10.**i, 8), initial_eta]) in all_cdfs:\n",
    "                    r_granularity = 10.**i\n",
    "                if tuple([initial_r, round_to_sigfigs(initial_eta + 10.**i, 8)]) in all_cdfs:\n",
    "                    \n",
    "                    eta_granularity = 10.**i\n",
    "\n",
    "            for d in range(1, NUM_ITERS[group] + 1):\n",
    "                if d == 1:\n",
    "                    initial_r, initial_eta = master_df.loc[group, 'initial_r'], master_df.loc[group, f'initial_eta']\n",
    "                else:\n",
    "                    initial_r, initial_eta = master_df.loc[group, f'iter{d - 1}_r'], master_df.loc[group, f'iter{d - 1}_eta']\n",
    "                r_granularity = r_granularity * 10.0**(-d+1) \n",
    "                eta_granularity = eta_granularity * 10.0**(-d+1) \n",
    "\n",
    "                r_range = [round_to_sigfigs(x) for x in np.arange(initial_r - r_granularity, initial_r + r_granularity, r_granularity/10)]\n",
    "                eta_range = [round_to_sigfigs(x) for x in np.arange(initial_eta - eta_granularity, initial_eta + eta_granularity, eta_granularity/10)]\n",
    "                simple_add_cdfs(r_range, eta_range, dir = os.path.join(CWD, \"groupCDFs\"), folder_name=f'{GROUP}{group}', n_samples = NUM_SAMPLES_OPTIMIZE, debug=False, tail_bound=1e-5)\n",
    "                cdfs_dict = combine_pickles(os.path.join(CWD, \"groupCDFs\", f'{GROUP}{group}'))\n",
    "                \n",
    "\n",
    "                temp_df = create_kurt_var_ksstat_df(cdfs_dict)\n",
    "                temp_df['scale'] = np.clip(master_df.loc[group,'obs_var'] / temp_df['variance'], 0, 1e5)\n",
    "                temp_df['variance'] = temp_df['variance'] * temp_df['scale']\n",
    "                temp_df['kurtosis'] = temp_df['kurtosis'] * temp_df['scale']\n",
    "                ksstats, best_param, kstest_stat = gridsearch(sample, cdfs_dict, debug=True, scales= temp_df['scale'])\n",
    "                temp_df = add_tests_to_df(cdfs_df=temp_df, group=group, var_kurt_df=master_df, ksstats=ksstats)\n",
    "                \n",
    "                group_cdf_df_dict[group] = pd.concat([group_cdf_df_dict[group], temp_df])\n",
    "                master_df.loc[group, 'best_r'], master_df.loc[group, f'best_eta'] = best_param[0], best_param[1]\n",
    "                best_scale = temp_df.loc[(temp_df[\"r\"] ==  best_param[0])&(temp_df[\"eta\"] ==  best_param[1])][\"scale\"].iloc[0]\n",
    "                master_df.loc[group, 'best_scale'] = best_scale\n",
    "                master_df.loc[group, 'kstest_stat_best'] = kstest_stat\n",
    "                master_df.loc[group, f'iter{d}_r'], master_df.loc[group, f'iter{d}_eta'] = best_param[0], best_param[1]\n",
    "                master_df.loc[group, f'kstest_stat_iter{d}'] = kstest_stat\n",
    "                \n",
    "                \n",
    "                print(f\"Iter {d} {GROUP} {group} best parameters: {best_param, master_df.loc[group, f'kstest_stat_iter{d}']}\")\n",
    "\n",
    "            temp = group_cdf_df_dict[group]\n",
    "            master_df.loc[group, 'pass_all'] = np.any(temp['pass_kstest'] & temp['pass_var'] & temp['pass_kurt'])\n",
    "            temp_df = group_cdf_df_dict[group].sort_values(['r', 'eta'])\n",
    "            rEtaKsstats_dict[group] = [temp_df['r'], temp_df['eta'], temp_df['ksstat']]\n",
    "\n",
    "            eps = 0.5\n",
    "            filtered_df = temp_df[(temp_df['r'] > master_df.loc[group, 'initial_r'] - eps) & \n",
    "                            (temp_df['r'] < master_df.loc[group, 'initial_r'] + eps) &\n",
    "                            (temp_df['eta'] > master_df.loc[group, 'initial_eta'] - eps) &\n",
    "                            (temp_df['eta'] < master_df.loc[group, 'initial_eta'] + eps)]\n",
    "            cols = ['pass_var', 'pass_kstest', 'pass_kurt']\n",
    "\n",
    "            fig = combo_test_plot(filtered_df, cols, \n",
    "                                plot_name=f\"{GROUP.capitalize()} {group} zoomed in: {', '.join([col[5:].capitalize() for col in cols])}\",\n",
    "                                best_param=(master_df.loc[group, 'best_r'], master_df.loc[group, f'best_eta']))\n",
    "            fig.figure.savefig(os.path.join(CWD, \"plots\", f\"optimized_full_grid_search_combo_plot_layer{group}.jpg\"), bbox_inches = 'tight', dpi=600)\n",
    "\n",
    "            \n",
    "            \n",
    "    master_df['n_pval_0.05'] = master_df.apply(lambda row : find_n_fixed_pval_stat(row.loc['kstest_stat_best'], row.loc['total_samples']), axis = 1)    \n",
    "    master_df[['total_samples', 'initial_r', 'initial_eta', 'kstest_stat_initial', 'best_r', 'best_eta', 'kstest_stat_best', 'n_pval_0.05']].to_csv(os.path.join(CWD, \"CSVs\", 'optimized_params.csv'))\n",
    "    master_df.to_csv(os.path.join(CWD, \"CSVs\", 'master_df_optimized.csv'))\n",
    "    pd.to_pickle(rEtaKsstats_dict, os.path.join(CWD, \"cache\", 'rEtaKsstats_dict.pickle'))\n",
    "\n",
    "master_df = pd.read_csv(master_df_optimized_path, index_col = GROUP)\n",
    "master_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Empirical and Computed CDF/PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in GROUPS:\n",
    "    group_info = master_df.loc[group]\n",
    "    best_r = group_info['best_r']\n",
    "    best_eta = group_info['best_eta']\n",
    "    best_scale = group_info['best_scale']\n",
    "    fig = visualize_cdf_pdf(sample = group_data_map[group], \n",
    "                    params = (best_r, best_eta, best_scale), \n",
    "                    log_scale = True,\n",
    "                    group = group)\n",
    "    fig.savefig(os.path.join(CWD, \"plots\", f'compare_cdf_pdf_layer_{group}.jpg'), bbox_inches = 'tight', dpi = 600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing with Gaussian and Laplace Priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_func(sample, distro, *args, n_samples=200):\n",
    "    if distro == 'gaussian' or distro == 'normal':\n",
    "        def var_func(var):\n",
    "            cdf = scipy.stats.norm(scale=var).cdf\n",
    "            return compute_ksstat(sample, cdf)\n",
    "        return var_func\n",
    "    elif distro == 'laplace':\n",
    "        def var_func(var):\n",
    "            cdf = scipy.stats.laplace(scale=var).cdf\n",
    "            return compute_ksstat(sample, cdf)\n",
    "        return var_func\n",
    "    elif distro == 't':\n",
    "        def var_func(var):\n",
    "            cdf = scipy.stats.t(df=2, scale=var).cdf\n",
    "            return compute_ksstat(sample, cdf)\n",
    "        return var_func\n",
    "    elif distro == 'prior_r':\n",
    "        eta = args[0]\n",
    "        def r_func(r):\n",
    "            cdf = compute_prior_cdf(r, eta, n_samples=n_samples)\n",
    "            return compute_ksstat(sample, cdf)\n",
    "        return r_func\n",
    "    elif distro == 'prior_eta':\n",
    "        r = args[0]\n",
    "        def eta_func(eta):\n",
    "            cdf = compute_prior_cdf(r, eta, n_samples=n_samples)\n",
    "            return compute_ksstat(sample, cdf)\n",
    "        return eta_func\n",
    "    elif distro == 'prior':\n",
    "        def r_eta_func(params):\n",
    "            r = params[0]\n",
    "            eta = params[1]\n",
    "            cdf = compute_prior_cdf(r, eta, n_samples=n_samples, debug=False)\n",
    "            return compute_ksstat(sample, cdf)\n",
    "        return r_eta_func\n",
    "    elif distro == 'prior_with_scale':\n",
    "        def r_eta_scale_func(params):\n",
    "            r = params[0]\n",
    "            eta = params[1]\n",
    "            scale = params[2]\n",
    "            cdf = compute_prior_cdf(r = r, eta = eta, n_samples=n_samples, debug=False)\n",
    "            return compute_ksstat(sample / np.sqrt(scale), cdf)\n",
    "        return r_eta_scale_func\n",
    "\n",
    "    print(\"Please enter a valid argument for `distro`: 'gaussian', 'laplace', 'prior_r', 'prior_eta', 'prior','prior_with_scale', 't'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df_path = Path(os.path.join(CWD, \"CSVs\", 'master_df.csv'))\n",
    "\n",
    "if RERUN or not master_df_path.exists():\n",
    "    upper_bound = int(1e6)\n",
    "    for group in GROUPS:\n",
    "        norm_result = scipy.optimize.minimize_scalar(generate_func(group_data_map[group], 'gaussian'), method = 'bounded', bounds = (0, upper_bound))\n",
    "        laplace_result = scipy.optimize.minimize_scalar(generate_func(group_data_map[group], 'laplace'), method = 'bounded', bounds = (0, upper_bound))\n",
    "        t_result = scipy.optimize.minimize_scalar(generate_func(group_data_map[group], 't'), method = 'bounded', bounds = (0, upper_bound))\n",
    "        \n",
    "        master_df.loc[group, 'param_gaussian'] = round_to_sigfigs(norm_result['x'], 6)\n",
    "        master_df.loc[group, 'kstest_stat_gaussian'] = round_to_sigfigs(norm_result['fun'], 6)\n",
    "        master_df.loc[group, 'kstest_pval_gaussian'] = round_to_sigfigs(stats.kstwo(n=master_df.loc[group, 'total_samples']).sf(master_df.loc[group, 'kstest_stat_gaussian']), 6)\n",
    "\n",
    "        master_df.loc[group, 'param_laplace'] = round_to_sigfigs(laplace_result['x'], 6)\n",
    "        master_df.loc[group, 'kstest_stat_laplace'] = round_to_sigfigs(laplace_result['fun'], 6)\n",
    "        master_df.loc[group, 'kstest_pval_laplace'] = round_to_sigfigs(stats.kstwo(n=master_df.loc[group, 'total_samples']).sf(master_df.loc[group, 'kstest_stat_laplace']), 6)\n",
    "\n",
    "        master_df.loc[group, 'param_laplace'] = round_to_sigfigs(laplace_result['x'], 6)\n",
    "        master_df.loc[group, 'kstest_stat_laplace'] = round_to_sigfigs(laplace_result['fun'], 6)\n",
    "        master_df.loc[group, 'kstest_pval_laplace'] = round_to_sigfigs(stats.kstwo(n=master_df.loc[group, 'total_samples']).sf(master_df.loc[group, 'kstest_stat_laplace']), 6) \n",
    "\n",
    "        master_df.loc[group, 'param_t'] = round_to_sigfigs(t_result['x'], 6)\n",
    "        master_df.loc[group, 'kstest_stat_t'] = round_to_sigfigs(t_result['fun'], 6)\n",
    "        master_df.loc[group, 'kstest_pval_t'] = round_to_sigfigs(stats.kstwo(n=master_df.loc[group, 'total_samples']).sf(master_df.loc[group, 'kstest_stat_t']), 6) \n",
    "\n",
    "        master_df.loc[group, 'kstest_pval_gengamma'] = round_to_sigfigs(stats.kstwo(n=master_df.loc[group, 'total_samples']).sf(master_df.loc[group, 'kstest_stat_best']))\n",
    "\n",
    "    master_df.to_csv(os.path.join(CWD, \"CSVs\", 'master_df.csv'))\n",
    "    \n",
    "master_df = pd.read_csv(os.path.join(CWD, \"CSVs\", 'master_df.csv'), index_col = GROUP)\n",
    "master_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "master_df = pd.read_csv(os.path.join(os.getcwd(), \"CSVs\", 'master_df.csv'), index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_directory(os.path.join(CWD, \"groupCDFs\"))\n",
    "eng.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hbmv_backup2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
